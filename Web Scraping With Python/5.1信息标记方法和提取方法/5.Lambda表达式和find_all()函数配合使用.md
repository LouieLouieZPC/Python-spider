# Lambdaè¡¨è¾¾å¼å’Œbs4åº“çš„find_all()å‡½æ•°é…åˆä½¿ç”¨


## ä¸€ã€åŸç†æ˜¯ä»€ä¹ˆï¼Ÿï¼š
BeautifulSoupä¸­ï¼š  
find_all()å‡½æ•°ä¸­å¯ä»¥ä¼ å…¥å‡½æ•°ï¼Œæ­¤å‡½æ•°å¿…é¡»æŠŠä¸€ä¸ªæ ‡ç­¾å¯¹è±¡ä½œä¸ºå‚æ•°ï¼Œå¹¶ä¸”è¿”å›å¸ƒå°”ç±»å‹çš„ç»“æœã€‚   BeautifulSoupå°†æ‰€æœ‰å¯¹è±¡ä¸­çš„æ ‡ç­¾ä¼ å…¥æ­¤å‡½æ•°ï¼Œè¾“å‡ºç»“æœä¸º'True'çš„å€¼ï¼Œå‰”é™¤å…¶ä»–æ ‡ç­¾ã€‚  

## äºŒã€æ€ä¹ˆç†è§£ï¼Ÿï¼š
Lambdaè¡¨è¾¾å¼å’Œbs4åº“çš„find_all()å‡½æ•°é…åˆä½¿ç”¨ç›¸å½“äºä¸€ä¸ªè¿‡æ»¤å™¨ï¼ŒLambdaè¡¨è¾¾å¼åˆ™æ˜¯è¿‡æ»¤æ¡ä»¶ï¼ç±»ä¼¼äºlambdaè¡¨è¾¾å¼å’Œfilter()å‡½æ•°æ­é…ä½¿ç”¨



## ä¸‰ã€å®é™…æ“ä½œï¼š
```python
#!/usr/bin/env python3
#-*-coding:utf-8-*-

from fake_useragent import UserAgent
import requests
from bs4 import BeautifulSoup
url='http://www.pythonscraping.com/pages/page3.html'

ua=UserAgent()
kv={'user-agent':ua.random}


try:
    r=requests.get(url,timeout=30,headers=kv)
    print(r.status_code)
    r.raise_for_status()
    r.encoding=r.apparent_encoding
    demo=r.text
except:
    print('something is error!!!ğŸ˜±')
else:
    soup=BeautifulSoup(demo,'html.parser')
    print(soup.prettify())
    print('----------------------------------------------------------------')
    pr=soup.findAll(lambda tag:len(tag.attrs)==2)   #è¿™é‡Œå°†Lambdaè¡¨è¾¾å¼å’Œfind_all()å‡½æ•°é…åˆä½¿ç”¨
    print(pr)
```